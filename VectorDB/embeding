我来帮你说明如何使用 VLLM 部署 BGE-ZH-1.5 模型。让我一步一步解释:

```python
# 1. 首先安装必要的包
!pip install vllm transformers accelerate
```

以下是部署的主要代码:

```python
from vllm import LLM, SamplingParams

# 1. 初始化模型
model_name = "BAAI/bge-large-zh-v1.5"  # BGE中文模型
llm = LLM(
    model=model_name,
    tensor_parallel_size=1,  # GPU数量，单GPU设为1
    trust_remote_code=True,
    dtype="float16",  # 使用 float16 以节省显存
)

# 2. 设置采样参数
sampling_params = SamplingParams(
    temperature=0.7,
    top_p=0.95,
    max_tokens=1024
)

# 3. 构建API服务
from fastapi import FastAPI
from pydantic import BaseModel
import uvicorn

app = FastAPI()

class EmbeddingRequest(BaseModel):
    texts: list[str]

@app.post("/embeddings")
async def get_embeddings(request: EmbeddingRequest):
    try:
        # 获取嵌入向量
        embeddings = llm.embed(request.texts)
        return {"embeddings": embeddings.tolist()}
    except Exception as e:
        return {"error": str(e)}

# 4. 启动服务
if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

使用示例:
```python
# 测试API
import requests

texts = ["这是第一段文本", "这是第二段文本"]
response = requests.post(
    "http://localhost:8000/embeddings",
    json={"texts": texts}
)
print(response.json())
```

更完整的部署方案，包含错误处理和批处理:

```python
from vllm import LLM, SamplingParams
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import uvicorn
import numpy as np
from typing import List, Optional
import torch

class EmbeddingConfig:
    BATCH_SIZE = 32
    MAX_LENGTH = 512

class EmbeddingRequest(BaseModel):
    texts: List[str]
    batch_size: Optional[int] = EmbeddingConfig.BATCH_SIZE

class EmbeddingResponse(BaseModel):
    embeddings: List[List[float]]
    metadata: dict

class EmbeddingService:
    def __init__(self):
        self.model = LLM(
            model="BAAI/bge-large-zh-v1.5",
            tensor_parallel_size=1,
            trust_remote_code=True,
            dtype="float16",
            max_num_batched_tokens=4096,
        )
        
    def _batch_texts(self, texts: List[str], batch_size: int):
        """将文本分批处理"""
        for i in range(0, len(texts), batch_size):
            yield texts[i:i + batch_size]
    
    async def get_embeddings(self, texts: List[str], batch_size: int) -> np.ndarray:
        """获取文本嵌入向量"""
        all_embeddings = []
        
        for batch in self._batch_texts(texts, batch_size):
            try:
                # 获取当前批次的嵌入向量
                batch_embeddings = self.model.embed(batch)
                all_embeddings.extend(batch_embeddings)
            except Exception as e:
                raise HTTPException(
                    status_code=500,
                    detail=f"Embedding generation failed: {str(e)}"
                )
                
        return np.array(all_embeddings)

app = FastAPI()
embedding_service = EmbeddingService()

@app.post("/embeddings", response_model=EmbeddingResponse)
async def create_embeddings(request: EmbeddingRequest):
    try:
        # 验证输入
        if not request.texts:
            raise HTTPException(
                status_code=400,
                detail="No texts provided"
            )
            
        # 获取嵌入向量
        embeddings = await embedding_service.get_embeddings(
            request.texts,
            request.batch_size
        )
        
        # 准备响应
        response = {
            "embeddings": embeddings.tolist(),
            "metadata": {
                "model": "bge-large-zh-v1.5",
                "dimension": embeddings.shape[1],
                "num_texts": len(request.texts)
            }
        }
        
        return response
        
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"An error occurred: {str(e)}"
        )

@app.get("/health")
async def health_check():
    return {"status": "healthy"}

if __name__ == "__main__":
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8000,
        workers=1  # 设置工作进程数
    )
```

部署建议:

1. 硬件配置:
```bash
# 推荐配置
- GPU: >= 16GB 显存
- RAM: >= 32GB
- CPU: >= 8核
```

2. Docker部署:
```dockerfile
FROM nvidia/cuda:11.8.0-runtime-ubuntu20.04

# 设置工作目录
WORKDIR /app

# 安装Python和依赖
RUN apt-get update && apt-get install -y \
    python3 \
    python3-pip \
    && rm -rf /var/lib/apt/lists/*

# 复制需要的文件
COPY requirements.txt .
COPY app.py .

# 安装依赖
RUN pip3 install -r requirements.txt

# 暴露端口
EXPOSE 8000

# 启动服务
CMD ["python3", "app.py"]
```

3. 性能优化建议:
```python
# 1. 使用float16减少显存占用
llm = LLM(
    model=model_name,
    dtype="float16"
)

# 2. 开启批处理以提高吞吐量
batch_size = 32  # 根据显存大小调整

# 3. 使用缓存减少重复计算
from functools import lru_cache

@lru_cache(maxsize=1024)
def get_embedding(text: str):
    return llm.embed([text])[0]
```

4. 监控指标:
```python
from prometheus_client import Counter, Histogram

# 添加监控指标
embedding_requests = Counter('embedding_requests_total', 'Total embedding requests')
embedding_latency = Histogram('embedding_latency_seconds', 'Embedding generation latency')
```

这样就完成了一个基本的BGE中文模型的VLLM部署。记得根据实际需求调整配置参数，并添加适当的错误处理和监控。
