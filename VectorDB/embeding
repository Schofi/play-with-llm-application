我来帮你说明如何使用 VLLM 部署 BGE-ZH-1.5 模型。让我一步一步解释:

```python
# 1. 首先安装必要的包
!pip install vllm transformers accelerate
```

以下是部署的主要代码:

```python
from vllm import LLM, SamplingParams

# 1. 初始化模型
model_name = "BAAI/bge-large-zh-v1.5"  # BGE中文模型
llm = LLM(
    model=model_name,
    tensor_parallel_size=1,  # GPU数量，单GPU设为1
    trust_remote_code=True,
    dtype="float16",  # 使用 float16 以节省显存
)

# 2. 设置采样参数
sampling_params = SamplingParams(
    temperature=0.7,
    top_p=0.95,
    max_tokens=1024
)

# 3. 构建API服务
from fastapi import FastAPI
from pydantic import BaseModel
import uvicorn

app = FastAPI()

class EmbeddingRequest(BaseModel):
    texts: list[str]

@app.post("/embeddings")
async def get_embeddings(request: EmbeddingRequest):
    try:
        # 获取嵌入向量
        embeddings = llm.embed(request.texts)
        return {"embeddings": embeddings.tolist()}
    except Exception as e:
        return {"error": str(e)}

# 4. 启动服务
if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

使用示例:
```python
# 测试API
import requests

texts = ["这是第一段文本", "这是第二段文本"]
response = requests.post(
    "http://localhost:8000/embeddings",
    json={"texts": texts}
)
print(response.json())
```

更完整的部署方案，包含错误处理和批处理:

```python
from vllm import LLM, SamplingParams
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import uvicorn
import numpy as np
from typing import List, Optional
import torch

class EmbeddingConfig:
    BATCH_SIZE = 32
    MAX_LENGTH = 512

class EmbeddingRequest(BaseModel):
    texts: List[str]
    batch_size: Optional[int] = EmbeddingConfig.BATCH_SIZE

class EmbeddingResponse(BaseModel):
    embeddings: List[List[float]]
    metadata: dict

class EmbeddingService:
    def __init__(self):
        self.model = LLM(
            model="BAAI/bge-large-zh-v1.5",
            tensor_parallel_size=1,
            trust_remote_code=True,
            dtype="float16",
            max_num_batched_tokens=4096,
        )
        
    def _batch_texts(self, texts: List[str], batch_size: int):
        """将文本分批处理"""
        for i in range(0, len(texts), batch_size):
            yield texts[i:i + batch_size]
    
    async def get_embeddings(self, texts: List[str], batch_size: int) -> np.ndarray:
        """获取文本嵌入向量"""
        all_embeddings = []
        
        for batch in self._batch_texts(texts, batch_size):
            try:
                # 获取当前批次的嵌入向量
                batch_embeddings = self.model.embed(batch)
                all_embeddings.extend(batch_embeddings)
            except Exception as e:
                raise HTTPException(
                    status_code=500,
                    detail=f"Embedding generation failed: {str(e)}"
                )
                
        return np.array(all_embeddings)

app = FastAPI()
embedding_service = EmbeddingService()

@app.post("/embeddings", response_model=EmbeddingResponse)
async def create_embeddings(request: EmbeddingRequest):
    try:
        # 验证输入
        if not request.texts:
            raise HTTPException(
                status_code=400,
                detail="No texts provided"
            )
            
        # 获取嵌入向量
        embeddings = await embedding_service.get_embeddings(
            request.texts,
            request.batch_size
        )
        
        # 准备响应
        response = {
            "embeddings": embeddings.tolist(),
            "metadata": {
                "model": "bge-large-zh-v1.5",
                "dimension": embeddings.shape[1],
                "num_texts": len(request.texts)
            }
        }
        
        return response
        
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"An error occurred: {str(e)}"
        )

@app.get("/health")
async def health_check():
    return {"status": "healthy"}

if __name__ == "__main__":
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8000,
        workers=1  # 设置工作进程数
    )
```

部署建议:

1. 硬件配置:
```bash
# 推荐配置
- GPU: >= 16GB 显存
- RAM: >= 32GB
- CPU: >= 8核
```

2. Docker部署:
```dockerfile
FROM nvidia/cuda:11.8.0-runtime-ubuntu20.04

# 设置工作目录
WORKDIR /app

# 安装Python和依赖
RUN apt-get update && apt-get install -y \
    python3 \
    python3-pip \
    && rm -rf /var/lib/apt/lists/*

# 复制需要的文件
COPY requirements.txt .
COPY app.py .

# 安装依赖
RUN pip3 install -r requirements.txt

# 暴露端口
EXPOSE 8000

# 启动服务
CMD ["python3", "app.py"]
```

3. 性能优化建议:
```python
# 1. 使用float16减少显存占用
llm = LLM(
    model=model_name,
    dtype="float16"
)

# 2. 开启批处理以提高吞吐量
batch_size = 32  # 根据显存大小调整

# 3. 使用缓存减少重复计算
from functools import lru_cache

@lru_cache(maxsize=1024)
def get_embedding(text: str):
    return llm.embed([text])[0]
```

4. 监控指标:
```python
from prometheus_client import Counter, Histogram

# 添加监控指标
embedding_requests = Counter('embedding_requests_total', 'Total embedding requests')
embedding_latency = Histogram('embedding_latency_seconds', 'Embedding generation latency')
```

这样就完成了一个基本的BGE中文模型的VLLM部署。记得根据实际需求调整配置参数，并添加适当的错误处理和监控。




VLLM 提供了命令行部署的方式，非常简单。以下是具体步骤：

1. 首先安装 VLLM：
```bash
pip install vllm
```

2. 使用命令行启动服务：
```bash
# 基本用法
python -m vllm.entrypoints.openai.api_server \
    --model BAAI/bge-large-zh-v1.5 \
    --host 0.0.0.0 \
    --port 8000

# 更多参数配置
python -m vllm.entrypoints.openai.api_server \
    --model BAAI/bge-large-zh-v1.5 \
    --host 0.0.0.0 \
    --port 8000 \
    --tensor-parallel-size 1 \  # GPU数量
    --trust-remote-code true \
    --dtype float16 \          # 使用 float16 精度
    --max-num-batched-tokens 4096  # 最大批处理token数
```

3. 测试部署的服务：
```python
import openai

# 配置客户端
client = openai.OpenAI(
    base_url="http://localhost:8000/v1",  # VLLM 服务地址
    api_key="dummy"  # 随便填写，本地部署不需要真实的 API key
)

# 发送请求
def get_embedding(text):
    response = client.embeddings.create(
        model="BAAI/bge-large-zh-v1.5",  # 模型名称
        input=text,  # 输入文本
    )
    return response.data[0].embedding

# 测试
texts = ["这是一段测试文本", "另一段测试文本"]
embeddings = [get_embedding(text) for text in texts]
print(embeddings)
```

4. 如果你想用 Docker 部署：
```bash
# 拉取镜像
docker pull vllm/vllm-openai

# 运行容器
docker run --gpus all -p 8000:8000 \
    vllm/vllm-openai \
    --model BAAI/bge-large-zh-v1.5 \
    --host 0.0.0.0 \
    --port 8000 \
    --trust-remote-code true
```

5. 常用参数说明：
```bash
--model              # 模型路径或huggingface模型名
--host               # 服务器地址
--port               # 端口号
--tensor-parallel-size # GPU数量
--trust-remote-code  # 是否信任远程代码
--dtype             # 数据类型(float16/float32)
--max-num-batched-tokens # 最大批处理token数
--max-num-seqs      # 最大序列数
--gpu-memory-utilization # GPU显存利用率(0-1)
--quantization      # 量化设置(如果需要)
```

6. 部署建议：
```bash
# 单GPU部署
python -m vllm.entrypoints.openai.api_server \
    --model BAAI/bge-large-zh-v1.5 \
    --host 0.0.0.0 \
    --port 8000 \
    --dtype float16 \
    --max-num-batched-tokens 4096

# 多GPU部署
python -m vllm.entrypoints.openai.api_server \
    --model BAAI/bge-large-zh-v1.5 \
    --tensor-parallel-size 2 \  # 使用2张GPU
    --host 0.0.0.0 \
    --port 8000 \
    --dtype float16

# 低显存设置
python -m vllm.entrypoints.openai.api_server \
    --model BAAI/bge-large-zh-v1.5 \
    --host 0.0.0.0 \
    --port 8000 \
    --dtype float16 \
    --gpu-memory-utilization 0.8 \
    --max-num-batched-tokens 2048
```

7. 启动脚本示例（start_server.sh）：
```bash
#!/bin/bash

# 设置环境变量
export CUDA_VISIBLE_DEVICES=0  # 指定GPU

# 启动服务
python -m vllm.entrypoints.openai.api_server \
    --model BAAI/bge-large-zh-v1.5 \
    --host 0.0.0.0 \
    --port 8000 \
    --dtype float16 \
    --max-num-batched-tokens 4096 \
    --trust-remote-code true \
    >> vllm.log 2>&1 &  # 将日志输出到文件

# 打印进程ID
echo $! > vllm.pid
```

这种命令行部署方式比较简单直接，适合快速部署和测试。如果需要更复杂的功能（如自定义路由、认证等），可能需要考虑使用前面提到的 FastAPI 方式部署。
