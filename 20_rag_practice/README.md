# play-with-llm-application


# 大模型多模态应用学习项目

这个仓库包含了一系列关于多模态学习的实践项目和教程。从基础的 AI 助手功能到高级的多模态应用，这里有丰富的学习资源。

## 项目列表

本节课将深入探讨 RAG 索引（Indexing）流程中的分块（Chunking）策略和嵌入（Embedding）技术。文档数据（Documents）经过解析后，通过分块技术将信息内容划分为适当大小的文档片段（chunks），从而使 RAG 系统能够高效处理和精准检索这些片段信息。分块的本质在于依据一定逻辑或语义原则，将较长文本拆解为更小的单元。分块策略有多种，各有侧重，**选择适合特定场景的分块策略是提升 RAG 系统召回率的关键**。


## 学习建议

每个项目都旨在帮助您逐步深入了解多模态 AI 的不同方面。建议按顺序学习，每完成一个项目后，花些时间实践和思考所学内容。祝您学习愉快！
提升工程
提示工程
一个提示通常包含以下几类元素：指令（Instruction）：指明模型要执行的特定任务或操作。上下文（Context）：为模型提供额外信息或背景，可以帮助引导模型生成更准确的响应。输入数据（Input Data）：我们希望模型回答的问题或感兴趣的输入内容。输出指示符（Output Indicator）：指定模型的输出类型或格式，例如格式、是否要求生成代码、总结文本或回答具体问题。

RAG 效果评估在 RAG 系统的开发、优化与应用过程中，RAG 效果评估是其中不可或缺的一环。通过建立统一的评估标准，能够公平、客观地比较不同 RAG 系统及其优化方法，从而识别出最佳实践。这套评估体系不仅帮助开发者发现系统的优劣，亦为后续的改进提供了有力的参考依据。下面我介绍一种在真实应用场景中采用的 RAG 效果评估标准与方法。评估方式：大模型打分：通过使用大语言模型对 RAG 的输出进行自动评分。这类评估方式效率高，能够快速处理大规模的评估任务，但在准确性上可能受到模型本身偏差的影响。人工打分：由人类评审员对 RAG 的输出进行逐一打分。人工评估方式可以提供更为精确、细致的反馈，特别是在检测生成答案中的细微错误和幻觉时，但其耗时较长，成本较高。

评估指标：CR 检索相关性（Context Relevancy）：该指标用于测量检索到的信息与查询上下文的相关性。如果检索到的信息偏离了原始查询，后续的生成任务就会受到负面影响。AR 答案相关性（Answer Relevancy）：衡量生成答案与原始问题之间的相关性。该指标主要评估生成的答案是否能够解决用户的问题，且内容是否逻辑连贯。F 可信度（Faithfulness）：评估生成的答案中是否存在幻觉（hallucination）或不准确之处。这一指标尤为重要，因为虚假的答案会极大影响用户对 RAG 系统的信任度。
RAG 效果评估是 RAG 系统完成搭建后的一个持续优化流程。通过设定打分标准和评估指标，综合评分能够准确反映 RAG 系统的整体性能。针对不同的应用场景，还可以引入更多评估指标，如 Top5 召回率、Top3 召回率、Top1 召回率以及其他常用的 NLP 评估指标。通过灵活组合这些评估方式与指标，可以更加精确地衡量 RAG 系统在特定场景中的表现，并为后续优化提供方向
处理多种文档格式、版面布局及阅读顺序还原的高精度、高效率文档解析技术，适用于特定场景的多样化分块策略，综合考虑特定领域精度、效率和文本块长度的嵌入模型，支持高效索引、检索和存储的向量数据库，结合多种检索技术的混合检索方法，以及能够捕捉查询词与文档块相关性的重排序技术。每个技术的细节优化都可以进一步提升整体检索精度。
## 许可证

请查看仓库中的 LICENSE 文件了解详细的许可信息。